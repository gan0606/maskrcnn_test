{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.models.detection import MaskRCNN_ResNet50_FPN_Weights\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備（Json書き換え）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # jsonファイルをフォルダ構成に合うように修正\n",
    "# # JSON ファイルのパス\n",
    "# json_path = \"data/annotations/train.json\"\n",
    "\n",
    "# # JSON を読み込む\n",
    "# with open(json_path, \"r\") as f:\n",
    "#     coco_data = json.load(f)\n",
    "\n",
    "# # `file_name` を修正\n",
    "# for img in coco_data[\"images\"]:\n",
    "#     img[\"file_name\"] = os.path.basename(img[\"file_name\"])  # `image/` を削除\n",
    "\n",
    "# # 修正後の JSON を保存\n",
    "# with open(json_path, \"w\") as f:\n",
    "#     json.dump(coco_data, f, indent=4)\n",
    "\n",
    "# print(\"JSON の修正が完了しました！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # jsonファイルをフォルダ構成に合うように修正\n",
    "# # JSON ファイルのパス\n",
    "# json_path = \"data/annotations/val.json\"\n",
    "\n",
    "# # JSON を読み込む\n",
    "# with open(json_path, \"r\") as f:\n",
    "#     coco_data = json.load(f)\n",
    "\n",
    "# # `file_name` を修正\n",
    "# for img in coco_data[\"images\"]:\n",
    "#     img[\"file_name\"] = os.path.basename(img[\"file_name\"])  # `image/` を削除\n",
    "\n",
    "# # 修正後の JSON を保存\n",
    "# with open(json_path, \"w\") as f:\n",
    "#     json.dump(coco_data, f, indent=4)\n",
    "\n",
    "# print(\"JSON の修正が完了しました！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JSON ファイルのパス\n",
    "# json_path = \"data/annotations/train.json\"\n",
    "\n",
    "# # JSON を読み込む\n",
    "# with open(json_path, \"r\") as f:\n",
    "#     coco_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # カテゴリidを0から1に変更\n",
    "# # torchでは0番は背景に使用される\n",
    "# for cat in coco_data['categories']:\n",
    "#     if cat['id'] == 0:\n",
    "#         cat['id'] = 1\n",
    "\n",
    "# # annotationsのcategory_idを0から1に変更\n",
    "# for ann in coco_data['annotations']:\n",
    "#     if ann['category_id'] == 0:\n",
    "#         ann['category_id'] = 1\n",
    "\n",
    "# # 保存\n",
    "# with open(json_path, 'w') as f:\n",
    "#     json.dump(coco_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # JSON ファイルのパス\n",
    "# json_path = \"data/annotations/val.json\"\n",
    "\n",
    "# # JSON を読み込む\n",
    "# with open(json_path, \"r\") as f:\n",
    "#     coco_data = json.load(f)\n",
    "\n",
    "# # カテゴリidを0から1に変更\n",
    "# # torchでは0番は背景に使用される\n",
    "# for cat in coco_data['categories']:\n",
    "#     if cat['id'] == 0:\n",
    "#         cat['id'] = 1\n",
    "\n",
    "# # annotationsのcategory_idを0から1に変更\n",
    "# for ann in coco_data['annotations']:\n",
    "#     if ann['category_id'] == 0:\n",
    "#         ann['category_id'] = 1\n",
    "\n",
    "# # 保存\n",
    "# with open(json_path, 'w') as f:\n",
    "#     json.dump(coco_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #かくにん\n",
    "# # # JSON を読み込む \n",
    "# with open(json_path, \"r\") as f: \n",
    "#     coco_data = json.load(f)\n",
    "    \n",
    "# for cat in coco_data['categories']: \n",
    "#     print(cat['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, root, image_dir, annotation_dir):\n",
    "#         # 初期化メソッドで必要なパラメータを設定\n",
    "#         self.root = root\n",
    "#         self.image_dir = image_dir\n",
    "#         self.annotation_file = os.path.join(annotation_dir, f\"{root}.json\")\n",
    "#         # COCOアノテーションファイルを読み込み\n",
    "#         self.coco = COCO(self.annotation_file)\n",
    "#         self.ids = list(self.coco.imgs.keys())\n",
    "\n",
    "#         # トレーニング用の変換とバリデーション用の変換を設定\n",
    "#         if root == \"train\":\n",
    "#             self.transforms = T.Compose([\n",
    "#                 T.RandomHorizontalFlip(0.5),  # 画像を水平反転する確率\n",
    "#                 T.RandomResizedCrop(size=(640, 640), scale=(0.8, 1.2)),  # 画像をランダムにリサイズ＆クロップ\n",
    "#                 T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 画像の色をランダムに調整\n",
    "#                 T.ToImage(),  # 画像をPIL形式に変換\n",
    "#                 T.ToDtype(torch.float32, scale=True)  # 画像をTensor形式に変換\n",
    "#             ])\n",
    "#         else:\n",
    "#             self.transforms = T.Compose([\n",
    "#                 T.ToImage(),  # 画像をPIL形式に変換\n",
    "#                 T.ToDtype(torch.float32, scale=True)  # 画像をTensor形式に変換\n",
    "#             ])\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # 指定されたインデックスに対応する画像のIDを取得\n",
    "#         img_id = self.ids[index]\n",
    "#         img_info = self.coco.imgs[img_id]\n",
    "#         img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "#         img = Image.open(img_path).convert(\"RGB\")  # 画像をRGB形式で読み込み\n",
    "\n",
    "#         # 画像に対応するアノテーションを取得\n",
    "#         ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "#         annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "#         boxes = []\n",
    "#         labels = []\n",
    "#         masks = []\n",
    "\n",
    "#         for ann in annotations:\n",
    "#             x_min, y_min, width, height = ann[\"bbox\"]\n",
    "#             x_max = x_min + width\n",
    "#             y_max = y_min + height\n",
    "\n",
    "#             # バウンディングボックスの幅と高さが正の値であることを確認\n",
    "#             if width > 0 and height > 0:\n",
    "#                 boxes.append([x_min, y_min, x_max, y_max])\n",
    "#                 labels.append(ann[\"category_id\"])\n",
    "#                 masks.append(self.coco.annToMask(ann))\n",
    "#             else:\n",
    "#                 # 無効なバウンディングボックスを検出\n",
    "#                 print(f\"Invalid bbox found: {ann['bbox']} for image ID {img_id} in file {img_path}\")\n",
    "\n",
    "#         if len(boxes) == 0:\n",
    "#             # 無効なアノテーションのみの場合、画像をスキップ\n",
    "#             return None\n",
    "\n",
    "#         # アノテーションをTensor形式に変換\n",
    "#         boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "#         labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "#         masks = torch.as_tensor(np.array(masks), dtype=torch.uint8)\n",
    "\n",
    "#         target = {\n",
    "#             \"boxes\": boxes,\n",
    "#             \"labels\": labels,\n",
    "#             \"masks\": masks,\n",
    "#             \"image_id\": torch.tensor([img_id]),\n",
    "#             \"area\": torch.as_tensor([ann[\"area\"] for ann in annotations if ann[\"area\"] > 0], dtype=torch.float32),\n",
    "#             \"iscrowd\": torch.as_tensor([ann[\"iscrowd\"] for ann in annotations], dtype=torch.int64),\n",
    "#         }\n",
    "\n",
    "#         # 画像に変換を適用\n",
    "#         img = self.transforms(img)\n",
    "#         return img, target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         # データセットのサイズを返す\n",
    "#         return len(self.ids)\n",
    "\n",
    "#     def collate_fn(self, batch):\n",
    "#         batch = list(filter(lambda x: x is not None, batch))\n",
    "#         return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット、データローダ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCocoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, image_dir, annotation_dir):\n",
    "        self.root = root\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_file = os.path.join(annotation_dir, f\"{root}.json\")\n",
    "        self.coco = COCO(self.annotation_file)\n",
    "        self.ids = list(self.coco.imgs.keys())\n",
    "\n",
    "        # 画像変換（train はデータ拡張あり、val はなし）\n",
    "        if root == \"train\":\n",
    "            self.transforms = T.Compose([\n",
    "                T.RandomHorizontalFlip(0.5),\n",
    "                T.RandomResizedCrop(size=(640, 640), scale=(0.8, 1.2)),\n",
    "                T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                T.ToImage(),\n",
    "                T.ToDtype(torch.float32, scale=True)\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = T.Compose([\n",
    "                T.ToImage(),\n",
    "                T.ToDtype(torch.float32, scale=True)\n",
    "            ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.ids[index]\n",
    "        img_info = self.coco.imgs[img_id]\n",
    "        img_path = os.path.normpath(os.path.join(self.image_dir, img_info['file_name']))\n",
    "\n",
    "        # 画像の読み込み\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"⚠ 画像が見つかりません: {img_path}\")\n",
    "            return None  # エラー回避\n",
    "\n",
    "        # アノテーション取得\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "        annotations = self.coco.loadAnns(ann_ids)\n",
    "\n",
    "        boxes, labels, masks, areas, iscrowd = [], [], [], [], []\n",
    "\n",
    "        for ann in annotations:\n",
    "            x_min, y_min, width, height = ann[\"bbox\"]\n",
    "            if width > 0 and height > 0:  # 無効な bbox を除外\n",
    "                x_max, y_max = x_min + width, y_min + height\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "                labels.append(ann[\"category_id\"])\n",
    "                masks.append(self.coco.annToMask(ann))\n",
    "                areas.append(ann[\"area\"])\n",
    "                iscrowd.append(ann[\"iscrowd\"])\n",
    "\n",
    "        # すべての bbox が無効なら空のターゲットを返す\n",
    "        if len(boxes) == 0:\n",
    "            return None\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"masks\": torch.tensor(np.array(masks), dtype=torch.uint8),\n",
    "            \"image_id\": torch.tensor([img_id]),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        img = self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        batch = list(filter(lambda x: x is not None, batch))\n",
    "        return tuple(zip(*batch)) if batch else ([], [])  # 空データ対応\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 評価関数の定義\n",
    "# def evaluate(model, data_loader, device, threshold=0.3):\n",
    "#     # モデルを評価モードに設定\n",
    "#     model.eval()\n",
    "#     coco_gt = data_loader.dataset.coco\n",
    "#     coco_results = []\n",
    "\n",
    "#     # 勾配計算をオフにして推論を行う\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in data_loader:\n",
    "#             # 画像をデバイスに転送\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             outputs = model(images)\n",
    "\n",
    "#             for target, output in zip(targets, outputs):\n",
    "#                 image_id = int(target[\"image_id\"].item())\n",
    "\n",
    "#                 for box, score, label in zip(output[\"boxes\"], output[\"scores\"], output[\"labels\"]):\n",
    "#                     if score < threshold:\n",
    "#                         continue\n",
    "#                     x_min, y_min, x_max, y_max = box.tolist()\n",
    "#                     width, height = x_max - x_min, y_max - y_min\n",
    "#                     coco_results.append({\n",
    "#                         \"image_id\": image_id,\n",
    "#                         \"category_id\": int(label.item()),\n",
    "#                         \"bbox\": [x_min, y_min, width, height],\n",
    "#                         \"score\": float(score.item())\n",
    "#                     })\n",
    "\n",
    "#     # 結果が空の場合はゼロを返す\n",
    "#     if not coco_results:\n",
    "#         return 0, 0, 0\n",
    "\n",
    "#     # COCOの評価関数を使用して結果を評価\n",
    "#     coco_dt = coco_gt.loadRes(coco_results)\n",
    "#     coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n",
    "#     coco_eval.evaluate()\n",
    "#     coco_eval.accumulate()\n",
    "#     coco_eval.summarize()\n",
    "\n",
    "#     # 評価指標を返す (AP@IoU=0.5:0.95, AP@IoU=0.5, AR@IoU=0.5:0.95)\n",
    "#     return coco_eval.stats[0], coco_eval.stats[1], coco_eval.stats[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate(model, data_loader, device, threshold=0.3):\n",
    "#     \"\"\"\n",
    "#     COCO フォーマットを使用して Mask R-CNN の bbox（バウンディングボックス）と\n",
    "#     segm（セグメンテーション）の両方の評価を行う。\n",
    "#     \"\"\"\n",
    "#     model.eval()\n",
    "#     coco_gt = data_loader.dataset.coco  # Ground Truth (GT)\n",
    "#     coco_results_bbox = []  # バウンディングボックス評価用\n",
    "#     coco_results_segm = []  # セグメンテーション評価用\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in data_loader:\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             outputs = model(images)\n",
    "\n",
    "#             for target, output in zip(targets, outputs):\n",
    "#                 image_id = int(target[\"image_id\"].item())\n",
    "\n",
    "#                 # bbox の評価\n",
    "#                 for box, score, label in zip(output[\"boxes\"], output[\"scores\"], output[\"labels\"]):\n",
    "#                     if score < threshold:\n",
    "#                         continue\n",
    "#                     x_min, y_min, x_max, y_max = box.tolist()\n",
    "#                     width, height = x_max - x_min, y_max - y_min\n",
    "#                     coco_results_bbox.append({\n",
    "#                         \"image_id\": image_id,\n",
    "#                         \"category_id\": int(label.item()),\n",
    "#                         \"bbox\": [x_min, y_min, width, height],\n",
    "#                         \"score\": float(score.item())\n",
    "#                     })\n",
    "\n",
    "#                 # segm の評価\n",
    "#                 for mask, score, label in zip(output[\"masks\"], output[\"scores\"], output[\"labels\"]):\n",
    "#                     if score < threshold:\n",
    "#                         continue\n",
    "#                     mask = mask[0].cpu().numpy()  # (1, H, W) → (H, W)\n",
    "#                     mask = (mask > 0.5).astype(np.uint8)  # バイナリマスク化\n",
    "#                     coco_results_segm.append({\n",
    "#                         \"image_id\": image_id,\n",
    "#                         \"category_id\": int(label.item()),\n",
    "#                         \"segmentation\": mask.tolist(),  # COCO は RLE (Run-Length Encoding) を期待する\n",
    "#                         \"score\": float(score.item())\n",
    "#                     })\n",
    "\n",
    "#     # bbox の評価\n",
    "#     if coco_results_bbox:\n",
    "#         coco_dt_bbox = coco_gt.loadRes(coco_results_bbox)\n",
    "#         coco_eval_bbox = COCOeval(coco_gt, coco_dt_bbox, \"bbox\")\n",
    "#         coco_eval_bbox.evaluate()\n",
    "#         coco_eval_bbox.accumulate()\n",
    "#         coco_eval_bbox.summarize()\n",
    "#         mAP_bbox = coco_eval_bbox.stats[0]  # mAP@IoU=0.5:0.95\n",
    "#     else:\n",
    "#         mAP_bbox = 0\n",
    "\n",
    "#     # segm の評価\n",
    "#     if coco_results_segm:\n",
    "#         coco_dt_segm = coco_gt.loadRes(coco_results_segm)\n",
    "#         coco_eval_segm = COCOeval(coco_gt, coco_dt_segm, \"segm\")\n",
    "#         coco_eval_segm.evaluate()\n",
    "#         coco_eval_segm.accumulate()\n",
    "#         coco_eval_segm.summarize()\n",
    "#         mAP_segm = coco_eval_segm.stats[0]  # mAP@IoU=0.5:0.95\n",
    "#     else:\n",
    "#         mAP_segm = 0\n",
    "\n",
    "#     return mAP_bbox, mAP_segm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, device, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Mask R-CNN の bbox（バウンディングボックス）と segm（セグメンテーション）評価を行い、\n",
    "    mAP, Precision, Recall を計算する。\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    coco_gt = data_loader.dataset.coco  # Ground Truth (GT)\n",
    "    coco_results_bbox = []  # bbox の評価用リスト\n",
    "    coco_results_segm = []  # segm の評価用リスト\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for target, output in zip(targets, outputs):\n",
    "                image_id = int(target[\"image_id\"].item())\n",
    "\n",
    "                # bbox の評価\n",
    "                for box, score, label in zip(output[\"boxes\"], output[\"scores\"], output[\"labels\"]):\n",
    "                    if score < threshold:\n",
    "                        continue\n",
    "                    x_min, y_min, x_max, y_max = box.tolist()\n",
    "                    width, height = x_max - x_min, y_max - y_min\n",
    "                    coco_results_bbox.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": int(label.item()),\n",
    "                        \"bbox\": [x_min, y_min, width, height],\n",
    "                        \"score\": float(score.item())\n",
    "                    })\n",
    "\n",
    "                # segm の評価\n",
    "                for mask, score, label in zip(output[\"masks\"], output[\"scores\"], output[\"labels\"]):\n",
    "                    if score < threshold:\n",
    "                        continue\n",
    "                    mask = mask[0].cpu().numpy()  # (1, H, W) → (H, W)\n",
    "                    mask = (mask > 0.5).astype(np.uint8)  # バイナリマスク化\n",
    "                    coco_results_segm.append({\n",
    "                        \"image_id\": image_id,\n",
    "                        \"category_id\": int(label.item()),\n",
    "                        \"segmentation\": mask.tolist(),  # COCO は RLE (Run-Length Encoding) を期待する\n",
    "                        \"score\": float(score.item())\n",
    "                    })\n",
    "\n",
    "    # bbox の評価\n",
    "    if coco_results_bbox:\n",
    "        coco_dt_bbox = coco_gt.loadRes(coco_results_bbox)\n",
    "        coco_eval_bbox = COCOeval(coco_gt, coco_dt_bbox, \"bbox\")\n",
    "        coco_eval_bbox.evaluate()\n",
    "        coco_eval_bbox.accumulate()\n",
    "        coco_eval_bbox.summarize()\n",
    "        mAP_bbox = coco_eval_bbox.stats[0]\n",
    "        precision_bbox = coco_eval_bbox.stats[1]  # Precision @ IoU=0.5\n",
    "        recall_bbox = coco_eval_bbox.stats[8]  # Recall @ IoU=0.5:0.95\n",
    "    else:\n",
    "        mAP_bbox, precision_bbox, recall_bbox = 0, 0, 0\n",
    "\n",
    "    # segm の評価\n",
    "    if coco_results_segm:\n",
    "        coco_dt_segm = coco_gt.loadRes(coco_results_segm)\n",
    "        coco_eval_segm = COCOeval(coco_gt, coco_dt_segm, \"segm\")\n",
    "        coco_eval_segm.evaluate()\n",
    "        coco_eval_segm.accumulate()\n",
    "        coco_eval_segm.summarize()\n",
    "        mAP_segm = coco_eval_segm.stats[0]\n",
    "        precision_segm = coco_eval_segm.stats[1]\n",
    "        recall_segm = coco_eval_segm.stats[8]\n",
    "    else:\n",
    "        mAP_segm, precision_segm, recall_segm = 0, 0, 0\n",
    "\n",
    "    return mAP_bbox, precision_bbox, recall_bbox, mAP_segm, precision_segm, recall_segm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 訓練関数の定義\n",
    "# def train(model, train_loader, val_loader, device, num_epochs=10, lr=0.005, output_dir=\"output\"):\n",
    "#     # モデルをデバイスに転送\n",
    "#     model.to(device)\n",
    "#     # 最適化器を定義\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#     # 学習率スケジューラを定義\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "#     # 出力ディレクトリを作成\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     csv_path = os.path.join(output_dir, \"training_log.csv\")\n",
    "\n",
    "#     # ログ用のリストを作成\n",
    "#     log_data = {\n",
    "#         'epoch': [],\n",
    "#         'loss': [],\n",
    "#         'mAP': [],\n",
    "#         'precision': [],\n",
    "#         'recall': []\n",
    "#     }\n",
    "\n",
    "#     # エポックごとに訓練を行う\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0\n",
    "\n",
    "#         # バッチごとに訓練を行う\n",
    "#         for images, targets in train_loader:\n",
    "#             # 画像とターゲットをデバイスに転送\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#             # モデルに入力して損失を計算\n",
    "#             loss_dict = model(images, targets)\n",
    "#             losses = sum(loss for loss in loss_dict.values())\n",
    "#             epoch_loss += losses.item()\n",
    "\n",
    "#             # 勾配をゼロにしてバックプロパゲーションを実行\n",
    "#             optimizer.zero_grad()\n",
    "#             losses.backward()\n",
    "#             optimizer.step()\n",
    "#         # 学習率の更新\n",
    "#         lr_scheduler.step()\n",
    "\n",
    "#         # 評価を行い、mAP、Precision、Recallを取得\n",
    "#         mAP, precision, recall = evaluate(model, val_loader, device)\n",
    "#         print(f\"Epoch {epoch+1}: Loss {epoch_loss:.4f}, mAP {mAP:.4f}, Precision {precision:.4f}, Recall {recall:.4f}\")\n",
    "#         log_data['epoch'].append(epoch+1)\n",
    "#         log_data['loss'].append(epoch_loss)\n",
    "#         log_data['mAP'].append(mAP)\n",
    "#         log_data['precision'].append(precision)\n",
    "#         log_data['recall'].append(recall)\n",
    "        \n",
    "#     # ログ用のDataFrameを作成\n",
    "#     log_df = pd.DataFrame(log_data)\n",
    "        \n",
    "\n",
    "#     # モデルの状態を保存\n",
    "#     torch.save(model.state_dict(), os.path.join(output_dir, \"model.pth\"))\n",
    "#     log_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_loader, val_loader, device, num_epochs=2, lr=0.005, output_dir=\"output\"):\n",
    "#     model.to(device)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "#     lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "#     csv_path = os.path.join(output_dir, \"training_log.csv\")\n",
    "\n",
    "#     log_data = {\n",
    "#         'epoch': [],\n",
    "#         'loss': [],\n",
    "#         'mAP_bbox': [],\n",
    "#         'mAP_segm': []\n",
    "#     }\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()\n",
    "#         epoch_loss = 0\n",
    "\n",
    "#         for images, targets in train_loader:\n",
    "#             images = [img.to(device) for img in images]\n",
    "#             targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#             loss_dict = model(images, targets)\n",
    "#             losses = sum(loss for loss in loss_dict.values())\n",
    "#             epoch_loss += losses.item()\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             losses.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         lr_scheduler.step()\n",
    "\n",
    "#         # bbox と segm の両方の評価を実行\n",
    "#         mAP_bbox, mAP_segm = evaluate(model, val_loader, device)\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}: Loss {epoch_loss:.4f}, mAP_bbox {mAP_bbox:.4f}, mAP_segm {mAP_segm:.4f}\")\n",
    "#         log_data['epoch'].append(epoch+1)\n",
    "#         log_data['loss'].append(epoch_loss)\n",
    "#         log_data['mAP_bbox'].append(mAP_bbox)\n",
    "#         log_data['mAP_segm'].append(mAP_segm)\n",
    "\n",
    "#     log_df = pd.DataFrame(log_data)\n",
    "#     torch.save(model.state_dict(), os.path.join(output_dir, \"model.pth\"))\n",
    "#     log_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, device, num_epochs=2, lr=0.005, output_dir=\"output\"):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    csv_path = os.path.join(output_dir, \"training_log.csv\")\n",
    "\n",
    "    log_data = {\n",
    "        'epoch': [],\n",
    "        'loss': [],\n",
    "        'mAP_bbox': [],\n",
    "        'precision_bbox': [],\n",
    "        'recall_bbox': [],\n",
    "        'mAP_segm': [],\n",
    "        'precision_segm': [],\n",
    "        'recall_segm': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for images, targets in train_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            epoch_loss += losses.item()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # bbox と segm の両方の評価を実行\n",
    "        mAP_bbox, precision_bbox, recall_bbox, mAP_segm, precision_segm, recall_segm = evaluate(model, val_loader, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss {epoch_loss:.4f}, mAP_bbox {mAP_bbox:.4f}, Precision_bbox {precision_bbox:.4f}, Recall_bbox {recall_bbox:.4f}, mAP_segm {mAP_segm:.4f}, Precision_segm {precision_segm:.4f}, Recall_segm {recall_segm:.4f}\")\n",
    "\n",
    "        log_data['epoch'].append(epoch+1)\n",
    "        log_data['loss'].append(epoch_loss)\n",
    "        log_data['mAP_bbox'].append(mAP_bbox)\n",
    "        log_data['precision_bbox'].append(precision_bbox)\n",
    "        log_data['recall_bbox'].append(recall_bbox)\n",
    "        log_data['mAP_segm'].append(mAP_segm)\n",
    "        log_data['precision_segm'].append(precision_segm)\n",
    "        log_data['recall_segm'].append(recall_segm)\n",
    "\n",
    "    log_df = pd.DataFrame(log_data)\n",
    "    torch.save(model.state_dict(), os.path.join(output_dir, \"model.pth\"))\n",
    "    log_df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練画像データへのパス\n",
    "image_dir = './data/images'\n",
    "annotation_dir = './data/annotations'\n",
    "batch_size = 16\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.83s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# 訓練用のデータセットを作成\n",
    "train_dataset = CustomCocoDataset(root='train', image_dir=os.path.join(image_dir, 'train'), annotation_dir=annotation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練用のデータローダを作成\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "バッチ内の画像数: 16\n",
      "ターゲットのキー: dict_keys(['boxes', 'labels', 'masks', 'image_id', 'area', 'iscrowd'])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader から最初のバッチを取得\n",
    "data_iter = iter(train_loader)\n",
    "images, targets = next(data_iter)\n",
    "\n",
    "# 画像の数とターゲットのキーを表示\n",
    "print(f\"バッチ内の画像数: {len(images)}\")\n",
    "print(f\"ターゲットのキー: {targets[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# 評価用のデータセットを作成\n",
    "val_dataset = CustomCocoDataset(root='val', image_dir=os.path.join(image_dir, 'val'), annotation_dir=annotation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評価用のデータローダを作成\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "バッチ内の画像数: 16\n",
      "ターゲットのキー: dict_keys(['boxes', 'labels', 'masks', 'image_id', 'area', 'iscrowd'])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader から最初のバッチを取得\n",
    "data_iter = iter(val_loader)\n",
    "images, targets = next(data_iter)\n",
    "\n",
    "# 画像の数とターゲットのキーを表示\n",
    "print(f\"バッチ内の画像数: {len(images)}\")\n",
    "print(f\"ターゲットのキー: {targets[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# デバイスの確認\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練済みのmaskrcnnをロード\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# クラス数を取得（背景クラス 0 を含むため +1）\n",
    "num_classes = len(train_dataset.coco.cats) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# 訓練済みモデルの出力ヘッドをデータセットのクラス数に合わせて付け替える\n",
    "# \n",
    "# box_predictorの置き換え\n",
    "in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features_box, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `mask_predictor` の置き換え\n",
    "in_channels_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_channels_mask, 256, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練の実行\n",
    "train(model, train_loader, val_loader, device, num_epochs=num_epochs, lr=0.005, output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのロード\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1)\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box_predictorの置き換え\n",
    "in_features_box = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features_box, num_classes)\n",
    "\n",
    "# `mask_predictor` の置き換え\n",
    "in_channels_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "model.roi_heads.mask_predictor = torchvision.models.detection.mask_rcnn.MaskRCNNPredictor(in_channels_mask, 256, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練済みモデルをロード\n",
    "model.load_state_dict(torch.load(\"output/model.pth\", map_location=device))\n",
    "# モデルをGPUに送る\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論対象の画像\n",
    "image_dir = \"data/images/test\"\n",
    "image_list = os.listdir(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論用の関数\n",
    "def infer_images(model, image_dir, image_list, device, threshold=0.5, output_dir=\"output\"):\n",
    "    \"\"\"\n",
    "    訓練済みの Mask R-CNN モデルで複数の画像を推論し、結果を保存する\n",
    "\n",
    "    Args:\n",
    "        model: 訓練済みの PyTorch モデル\n",
    "        image_dir: 画像のディレクトリ\n",
    "        image_list: 推論する画像ファイルのリスト\n",
    "        device: \"cuda\" または \"cpu\"\n",
    "        threshold: バウンディングボックスを表示する信頼度の閾値\n",
    "        output_dir: 推論結果の保存先ディレクトリ\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    transform = T.Compose([\n",
    "                T.ToImage(),  # 画像をPIL形式に変換\n",
    "                T.ToDtype(torch.float32, scale=True)\n",
    "                ]) \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for image_file in image_list:\n",
    "        image_path = os.path.join(image_dir, image_file)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)  # (C, H, W) → (1, C, H, W)\n",
    "\n",
    "        # 推論\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)[0]  # バッチの 0 番目を取得\n",
    "\n",
    "        # 画像を NumPy 配列に変換（可視化用）\n",
    "        image_np = np.array(image).astype(np.uint8)\n",
    "\n",
    "        # バウンディングボックスとラベルを描画\n",
    "        for i in range(len(output[\"boxes\"])):\n",
    "            score = output[\"scores\"][i].item()\n",
    "            if score < threshold:\n",
    "                continue  # 信頼度が閾値未満ならスキップ\n",
    "\n",
    "            box = output[\"boxes\"][i].cpu().numpy().astype(int)\n",
    "            label = output[\"labels\"][i].item()\n",
    "            mask = output[\"masks\"][i, 0].cpu().numpy()\n",
    "\n",
    "            # バウンディングボックスを描画\n",
    "            cv2.rectangle(image_np, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)\n",
    "            cv2.putText(image_np, f\"Label {label}: {score:.2f}\", \n",
    "                        (box[0], box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                        0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # マスクを適用（しきい値 0.5 で二値化）\n",
    "            mask_bin = (mask > 0.5).astype(np.uint8) * 255\n",
    "            mask_bin = cv2.applyColorMap(mask_bin, cv2.COLORMAP_JET)\n",
    "            image_np = cv2.addWeighted(image_np, 0.7, mask_bin, 0.3, 0)\n",
    "\n",
    "        # 結果を保存\n",
    "        output_path = os.path.join(output_dir, f\"output_{image_file}\")\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"Saved: {output_path}\")\n",
    "\n",
    "        # 結果を表示\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(image_np)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論の実行\n",
    "infer_images(model, image_dir, image_list, device, threshold=0.5, output_dir=\"output_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
